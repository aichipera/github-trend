> **【编者按】**
> 我们习惯于将AI的飞速进步归功于算法的革新与思想的突破，但一篇引人深思的文章及其讨论却提出了一个颠覆性观点：过去十五年的关键飞跃，从图像识别到大语言模型，其背后真正的驱动力并非新思想，而是新数据集的解锁。然而，这种“数据为王”的叙事能解释一切吗？当AI在特定任务上达到超人水平，却在面对新游戏时束手无策，这暴露了“智能”与“专精”的深刻鸿沟。本文将带您深入这场关于数据、泛化与智能本质的辩论，探讨AI的下一个奇点，究竟是源于下一个万亿级的数据金矿，还是需要一次通往真正“理解力”的认知觉醒。

# [AI的“蛮力增长”到头了？我们需要的不是更多数据，而是真正的理解力](20250702-there-are-no-new-ideas-in-ai-only.mp3)

人工智能正以一种近乎“摩尔定律”的姿态狂奔，模型迭代的速度让人目不暇接。我们普遍认为，这是全球顶尖实验室里天才们思想火花碰撞的结果。然而，Jack Morris在一篇广为流传的博客文章《[AI没有新思想，只有新数据集](https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only)》中，提出了一个更为朴素甚至有些刺耳的观点：推动AI发展的不是精妙绝伦的新算法，而是规模庞大、前所未有的新数据集。

## **数据为王：一部由数据集书写的AI简史**

Morris回溯了AI发展的四个里程碑式突破，并指出每一个都与一个全新数据源的规模化利用紧密相连：

1.  **深度神经网络（DNNs）的兴起**：2012年的AlexNet之所以能一鸣惊人，关键在于它成功“解锁”了ImageNet——一个拥有海量标注图像的数据库。数据，而非架构本身，点燃了计算机视觉的革命。
2.  **Transformer与大语言模型（LLMs）**：2017年《Attention Is All You Need》提出的Transformer架构，之所以成为范式，是因为它让模型能够有效消化“整个互联网”的文本数据。是“网络”这个前所未有的数据集，喂养出了BERT和GPT。
3.  **从指令遵从到对话（RLHF）**：ChatGPT的诞生归功于InstructGPT论文中提出的“基于人类反馈的强化学习”（RLHF）。这本质上是解锁了一种新的数据源：人类对“好文本”的主观偏好与判断。
4.  **推理能力的萌芽**：最新的进展，如OpenAI的O1，被认为是通过学习“验证器”（Verifiers）——如计算器、编译器等能评估输出正确性的工具——的数据来实现的。

这个论点极具说服力，因为它揭示了一个残酷的现实：许多被我们奉为圭臬的理论基础，如监督学习和强化学习，其核心思想早在上世纪90年代甚至更早就已存在。我们似乎只是在用旧瓶装新酒——而“新酒”就是源源不断的数据。这引出了一个推论：AI的下一个奇点，或许并非来自某个颠覆性的算法，而是当我们成功驾驭下一个海量数据源时——比如YouTube的视频流，或是来自现实世界机器人的感官数据。

## **泛化能力的“阿喀琉斯之踵”**

然而，在[Hacker News等社区的激烈讨论](https://news.ycombinator.com/item?id=44423983)中，一个尖锐的反对声音浮出水面，直指“数据为王”理论的软肋——**泛化能力**。

传奇程序员约翰·卡马克（John Carmack）的实验就是一盆冷水。他训练模型在某个2D游戏中达到超人水平，但当把这个“高手”模型放到一个它从未见过的新游戏里时，它的表现甚至比从零开始训练还差，出现了“负向迁移”。这有力地表明，AI学会的并非通用的“游戏玩法”，而是在特定规则下的最优解“肌肉记忆”。它没有形成真正的智能，只获得了狭隘的专精。

这与人类的学习方式形成了鲜明对比。一个玩家在《塞尔达传说》中能迅速上手，并不是因为他见过这个游戏的数据，而是因为他调动了大量的背景知识：他知道“剑”是用来攻击的，“钥匙”能开门，整个故事符合“英雄救美”的神话叙事母题。这种基于常识、文化和高阶概念的迁移能力，是当前AI模型望尘莫及的。

## **真正的“通用”智能，存在于何处？**

一位评论者提出了一个精妙的思想实验：如果将一款你熟悉的平台游戏（如《超级马里奥》）的画面进行傅里叶变换，你还能玩吗？大概率不能。尽管游戏的核心逻辑完全没变，但它的“表征”方式变了。

这揭示了人类智能的本质：我们的泛化能力并非无限，而是深深植根于我们所处世界的物理规律——尤其是“局部性原理”（locality）。我们能快速理解新事物，是因为它们大多遵循相似的物理和因果逻辑。AI之所以在泛化上步履维艰，或许正是因为它缺乏一个基于物理现实的、稳固的“世界模型”，它只是在庞大的数据海洋中学习相关性，而非因果性。

## **结语：超越数据，奔向理解**

“数据为王”的观点部分是正确的，它解释了AI为何能实现“蛮力增长”。但它也掩盖了一个更深层次的问题：没有真正的理解，再多的数据也可能只是在构建一个更精致的“拟合函数”，一个更会“鹦鹉学舌”的系统。

未来的突破，可能确实需要解锁视频、机器人感官等多模态数据。但比数据本身更重要的，是开发出能够从这些数据中构建出抽象、因果、可迁移的“世界模型”的新方法。我们不仅需要能“看懂”YouTube的AI，更需要能从中“理解”重力、情感和物理常识的AI。

或许，AI的进步曲线并非一条直线，而是一系列S型曲线的叠加。我们可能正处于当前数据驱动范式的收益递减阶段。下一轮指数级增长，需要的不仅仅是下一个ImageNet，更可能是一次类似于从炼金术到化学的科学革命——一次从“模式匹配”到真正“认知理解”的飞跃。
